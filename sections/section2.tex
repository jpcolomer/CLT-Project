A KWIK algorithm is an algorithm that, given a concept class $C$, parameters $(\varepsilon, \delta)$,  receives examples $x \in X$ that have been labeled by a target concept $c \in C$, it learns $c$.

A run for a KWIK algorithm works as follows.
\begin{enumerate}
  \item Receives an example $x \in X$
  \item Predicts $\hat{y} \in Y \cup \{\bot\}$
    \begin{itemize}
      \item If $\hat{y} \neq \bot$ and $|\hat{y} - c(x)| \leq \varepsilon$ it outputs $\hat{y}$. Otherwise, if $|\hat{y} - c(x)| > \varepsilon$ the algorithm failed.
      \item If $\hat{y} = \bot$ the algorithm updates the hypothesis using $c(x)$.
    \end{itemize}
\end{enumerate}

The probability of having a failed run must be upper bounded by $\delta$. Additionally, the total number of $\bot$ responses is upper bounded by $B(\varepsilon,\delta)=\text{poly}(1/\varepsilon,1/\delta)$.

KWIK framework has similarities with both $PAC$ and $OLMB$ models.
\begin{itemize}
  \item Similarly to PAC, KWIK algorithms must be $\varepsilon$-accurate with probability at least $1-\delta$. Nevertheless, inputs for the KWIK model come from an adversiary instead of a distribution like in PAC.
  \item In the same way KWIK relates to OLMB in the sense that inputs for both models are picked adversarially. However, instead of having a bound on the number of mistakes, KWIK has a bound on the number of $\bot$ outputs.
\end{itemize}

A KWIK algorithm can be easily converted into a OLMB algorithm by guessing a label instead of $\bot$. In this case, the resulting OLMB algorithm will have a mistake bound $B(\varepsilon,\delta)$.\\

Moreover, we saw in class that any OLMB algorithm can be used in order to construct a PAC algorithm. Therefore, any KWIK algorithm can be used to construct a PAC algorithm.


% TODO: Rename H to C

In this section, we will present a set of algorithms that, by combining KWIK
learners, are able to learn a more complex concept class.

\paragraph{Algorithm 6: Union Algorithm}
Let $H_i: X \to Y$ be a KWIK-learnable concept class by an algorithm $A_i$
with KWIK bound of $B_i(\varepsilon, \delta)$ for $1 \leq i \leq k$. Note that
all $H_1, \ldots, H_k$ share the same input and output spaces. \\

The union algorithm learns $H = \bigcup_{i=1}^k H_i$ with KWIK bound $B(\varepsilon,
\delta) = (k - 1) + \sum_{i = 1}^k B_i(\varepsilon, \delta)$. This can be
understood as a generalization of the Enumeration Algorithm (Algorithm 2). \\

The union algorithm maintains a set of ``potential algorithms'' $\hat{A}$,
initialized as $\{ A_1, \ldots, A_k \}$. For every received example
$x \in X$, it runs all algorithms in $\hat{A}$ to obtain a prediction from each
of them $\hat{y}_i$. It creates a set $\hat{L} = \{ \hat{y}_i | A_i \in \hat{A} \}$:
\begin{enumerate}
  \item If $\bot \in \hat{L}$, at least one of the algorithms doesn't know the
  answer with enough accuracy, then the union algorithm outputs $\bot$ and, once
  the correct output is received, it passes it to each of the algorithms that
  returned $\bot$.
  \item If $|\hat{L}| > 1$, at least two algorithms disagree on predicting the output of $x$.
  Then, the union algorithm outputs $\bot$. Once the correct output $y$ is
  received, it eliminates from $\hat{A}$ the algorithms for which $\hat{y}_i(x)
  \neq y$.
  \item Otherwise, $|\hat{L}| = 1$. This means that all the algorithms in
  $\hat{A}$ agree on the prediction of the output of $x$. The union algorithm outputs the element in
  $\hat{L}$.
\end{enumerate}

The algorithm will only output $\bot$ in the first or second cases described
above:
\begin{itemize}
  \item In the first case, the algorithm will return $\bot$ at most $\sum_{i = 1}^k
  B_i(\varepsilon, \delta)$ times.
  \item In the second case, the union algorithm is going to eliminate at least one
  algorithm from $\hat{A}$. Initially, $|\hat{A}| = k$, so the algorithm will
  output $\bot$ at most $k - 1$ times and eliminates $k - 1$ algorithms from
  $\hat{A}$.
\end{itemize}

In total, the KWIK bound of the union algorithm is the sum of both cases:
$$ B(\varepsilon, \delta) = (k - 1) + \sum_{i = 1}^k B_i(\varepsilon, \delta) $$


\paragraph{Example 2}
Let:
\begin{itemize}
  \item $X = Y = \mathbb{R}$
  \item $H_1 = \{ f \ | \ f(x) =|x - c|, c \in \mathbb{R} \}$ KWIK learnable by $A_1$
  \item $H_2 = \{ f \ | \ f(x) = mx + b, m \in \mathbb{R}, b \in \mathbb{R} \}$ KWIK learnable by $A_2$
  \item $H = H_1 \cup H_2$
\end{itemize}

$H_1$ can be learned using a 1 dimension version of Planar-distance Algorithm (Algorithm 3) with a
KWIK bound of 2. $H_2$ can be learned with a KWIK bound of $2$,
the number of points needed to define a line.
$H$ is KWIK learnable using the union algorithm.
\begin{enumerate}
  \item Initially, $\hat{A} = \{A_1, A_2 \}$
  \item First, the algorithm receives $x_1 = 2$. $\hat{L} = \{ \bot \}$ because none of the algorithms know the answer. Then, it outputs $\bot$ and receives $y_1 = 2$, which it passes to $A_1$, $A_2$.
  \item The algorithm receives $x_2 = 8$. $A_1$ and $A_2$ still don't know the actual value. Therefore, $\hat{L} = \{ \bot \}$ and it outputs $\bot$. Afterwards, it receives and passes $y_2 = 4$ to $A_1$ and $A_2$.
    Since $A_1$ and $A_2$ have returned $\bot$ twice, they have learned an hypothesis for every $x \in \mathbb{R}$.
    \begin{itemize}
      \item $A_1$ knows that $c = 4$. Because it's the only point that has a distance of $2$ from $x_1=2$ and 4 from $x_2 = 8$.
      \item $A_2$ learns the line that connects $(x_1, y_1)$ and $(x_2, y_2)$. Therefore, $m = 1/3$ and $b = 4/3$.
    \end{itemize}
  \item The algorithm receives $x_3 = 1$. $\hat{L} = \{ 3, 5/3 \}$
    \begin{itemize}
      \item $A_1$ predicts $3$
      \item $A_2$ predicts $5/3$
    \end{itemize}
    It returns $\bot$ because $A_1$ and $A_2$ disagree. Then it receives $y_3 = 3$ and eliminates algorithm $A_2$.
    From now on the algorithm will predict all the examples using $A_1$.
\end{enumerate}

\paragraph{Algorithm 7: Input-Partition Algorithm}
Let:
\begin{itemize}
  \item $X = \bigcup_{i=1}^k X_i$, where all $X_i \cap X_j = \emptyset \ \forall i \neq j$
  \item $H_i: X_i \to Y$ be a KWIK-learnable concept class by an algorithm $A_i$
  with KWIK bound of $B_i(\varepsilon, \delta)$ for $1 \leq i \leq k$. Note
  that all $H_1, \ldots, H_k$ share the same output space but their input spaces
  are disjoint.
  \item $H \subseteq X \to Y$ be a concept class.
\end{itemize}

The input-partition algorithm KWIK-learns $H$ with KWIK bound $$B(\varepsilon,
\delta) = \sum_{i=1}^k B_i(\varepsilon, \delta/k)$$

When the algorithm receives an $x \in X_i$, it calls $A_i$ with parameters
$\varepsilon$ and $\delta/k$ and returns its response. By definition of $A_i$,
its response will be $\varepsilon$-accurate with probability at least $1 -
\delta/k$. Using union bounds, we know that the response over all $X$ of the
algorithm will be $\varepsilon$-accurate with probability at least $1 - \delta$.
Given that $X$ is the union of $k$ disjoint input spaces, the KWIK bound
of the input-partition algorithm will be $B(\varepsilon, \delta) = \sum_{i=1}^k
= B_i(\varepsilon, \delta/k)$.

\paragraph{Example 3} Let $G$ be a Markov Decision Process consisting of $n$
states and $m$ actions. There are $n^2m$ transitions represented as triplets of
the form $(state_{origin}, action, state_{target})$. We want to learn the
probability of each transition to happen. By receiving Bernoulli observations,
we can KWIK-learn this using algorithm 7 (Input-Partition Algorithm) which
uses algorithm 4 (Coin-Learning Algorithm) as its $A_i$ for every $1 \leq i \leq
n^2m$ with parameters $\varepsilon$ and $\frac{\delta}{n^2m}$. The KWIK bound of
this algorithm is $$B(\varepsilon, \delta) = n^2m \cdot O\left(\frac{1}{\varepsilon^2}
\ln \frac{n^2m}{\delta}\right) = O\left(\frac{n^2m}{\varepsilon^2} \ln \frac{nm}{\delta}\right)$$

\paragraph{Algorithm 8: Cross-Product Algorithm}
Let:
\begin{itemize}
  \item $H_i \subseteq X_i \to Y_i$ be a KWIK learnable concept class with bound $B_i(\varepsilon, \delta)$ by using algorithm $A_i$ for $1 \leq i \leq k$
  \item $X = X_1 \times \cdots \times X_k$ be an input space.
  \item $Y = Y_1 \times \cdots \times Y_k$ be an output space.
  \item $H \subseteq X \to Y$
\end{itemize}

$H$ can be KWIK-learned by using cross-product algorithm with KWIK bound of $B(\varepsilon,\delta) =  \sum^{k}_{i=1}B_i(\varepsilon,\delta/k)$ \\

Each input $x$ received by the algorithm can be understood as a vector $[x_1,\ldots,x_k]$ with $x_i \in X_i$ and an output $y = [y_1, \ldots, y_k]$ with $y_i \in Y_i$. Therefore, the $i$-th component can be learned independently by using $A_i$
with parameters $\varepsilon, \delta/k$. \\

For each $x \in X$ received by the algorithm, it passes component $x_i$ to algorithm $A_i$, for $1 \leq i \leq k$. Each subalgorithm returns a prediction $\hat{y}_i \in Y_i \cup \{\bot\}$ for $x_i$.
\begin{itemize}
  \item  If any component returns $\bot$, the algorithm returns $\bot$ and receives the correct output $y$.
    Then it passes $y_j$ to $A_j$ that has returned $\bot$.
  \item Otherwise, the algorithm returns $\hat{y} = [\hat{y}_1, \ldots, \hat{y}_k]$
\end{itemize}

The result is $\varepsilon$ accurate in each component with probability greater than $1-\delta/k$. As a result, by union bounds the result is $\varepsilon$ accurate in all components with probability at least $1-\delta$.
Since each subalgorithm will output at most $B_i(\varepsilon,\delta/k)$ $\bot$s, the algorithm's KWIK bound is $B(\varepsilon, \delta) = \sum^{k}_{i=1} B_i(\varepsilon,\delta/k)$.

% TODO: \paragraph{Example 4}

\paragraph{Algorithm 9: Noisy Union Algorithm}
Let:
\begin{itemize}
  \item $X$ be the input space.
  \item $Y = [0, 1]$ be the output space.
  \item $Z = \{0, 1\}$ be the observation space.
  \item $H_i \subseteq X \to Y$ be a KWIK learnable concept class with bound
  $B_i(\varepsilon, \delta)$ by using algorithm $A_i$ for $1 \leq i \leq k$.
  \item $H = \bigcup_{i=1}^k H_i$
\end{itemize}

In this algorithm, the paper uses a definition of noise different from the one
we used in class. Here, in the noisy world, an algorithm receives observations
$z \in Z$, where $Pr[z = 1] = y$ and $Pr[z = 0] = 1 - y$. The algorithm wants
to learn $y$. \\

The noisy union algorithm can KWIK-learn $H$ with KWIK bound
$$B(\varepsilon, \delta) = O\left( \frac{k}{\varepsilon^2} \ln \frac{k}{\delta}
\right) + \sum_{i=1}^k B_i \left( \frac{\varepsilon}{4}, \frac{\delta}{k + 1}
\right)$$

For simplicity, we will only consider the case where $k = 2$, which can be
extended to the general case. For trial $t$, when the algorithm receives $x_t$,
it runs subalgorithms $A_1$ and $A_2$ with parameters $\varepsilon/4$ and
$\delta/3$. Let $\hat{L} = \{\hat{y}_{t1}, \hat{y}_{t2}\}$ be the set containing
both responses.
\begin{itemize}
  \item If $\bot \in \hat{L}$, the algorithm returns $\bot$ and obtains the
  observation $z_t$ and sends it to all the subalgorithms that returned $\bot$
  so that they can learn.
  \item If $\bot \notin \hat{L}$ we will consider two cases:
  \begin{itemize}
    \item If $|\hat{y}_{t1} - \hat{y}_{t2}| \leq \varepsilon$, then both predictions
    are close enough. Let $\hat{p}_t = (\hat{y}_{t1} + \hat{y}_{t2})/2 $. Then,
    with probability at least $1 - \delta/3$, $|\hat{p}_t - y_t| \leq \varepsilon$:
    \begin{align*}
    |\hat{p}_t - y_t| &= |\hat{p}_t - \hat{y}_{t1} + \hat{y}_{t1} - y_t| \\
    |\hat{p}_t - y_t| &\leq |\hat{p}_t - \hat{y}_{t1}| + |\hat{y}_{t1} - y_t| \\
                      &\leq |(\hat{y}_{t1} + \hat{y}_{t2})/2 - \hat{y}_{t1}| + |\hat{y}_{t1} - y_t| \\
                      &\leq |(\hat{y}_{t2} - \hat{y}_{t1})/2| + |\hat{y}_{t1} - y_t| \\
                      &\leq |\hat{y}_{t2} - \hat{y}_{t1}|/2 + |\hat{y}_{t1} - y_t| \\
                      &\leq \underbrace{|\hat{y}_{t2} - \hat{y}_{t1}|/2}_{\leq \varepsilon/2} + \underbrace{|y_{t1} - y_t|}_{\leq \varepsilon/4} \\
                      &\leq \varepsilon/2 + \varepsilon/4 \\
                      &\leq 3\varepsilon/4 \\
                      &\leq \varepsilon \\
    \end{align*}
    \item If $|\hat{y}_{t1} - \hat{y}_{t2}| > \varepsilon$ (both predictions
    are not close enough) the algorithm ``doesn't know'' an accurate-enough
    prediction, so it returns $\bot$ and needs to know which of the algorithms
    was wrong. However, differently from the noise-free union algorithm, it
    can't eliminate any of the algorithms on the basis of a single observation.
    Instead, it needs to observe this same behavior enough number of times to
    identify the wrong one. For that, it keeps track of $l_i = \sum_{t \in I}
    (\hat{y}_{ti} - z_t)$ for $i \in \{1, 2\}$ and $I = \{t | \ |\hat{y}_{t1} -
    \hat{y}_{t2}| > \varepsilon\}$. If $|I| \geq
    \frac{8}{\varepsilon^2} \ln \frac{3}{\delta} = O\left( \frac{1}{\varepsilon^2}
    \ln \frac{1}{\delta} \right)$, then the algorithm eliminates the subalgorithm
    whose $l_i$ is higher.
  \end{itemize}
\end{itemize}

Given that each of the subalgorithms outputs an $\varepsilon/4$-accurate hypothesis
with probability at least $1 - \delta/3$ and the elimination of the ``bad''
algorithm also succeeds with probability at least $1 - \delta/3$, then, by union
bounds, the overall algorithm gives an $\varepsilon$-accurate hypothesis with
probability at least $1 - \delta$. \\

The algorithm returns $\bot$ when:
\begin{enumerate}
  \item One of the subalgorithms reports $\bot$, which happens at most
  $\sum_{i = 1}^2 B_i(\varepsilon/4, \delta/3)$ times.
  \item Both algorithms' predictions are not close enough, which happens at most
  $|I| = O\left( \frac{1}{\varepsilon^2} \ln \frac{1}{\delta} \right)$ times.
\end{enumerate}
Overall, the algorithm's KWIK bound is:
$$B(\varepsilon, \delta) = O\left( \frac{1}{\varepsilon^2} \ln \frac{1}{\delta}
\right) + \sum_{i = 1}^2 B_i\left(\frac{\varepsilon}{4}, \frac{\delta}{3}\right)$$

The general case where $k > 2$ can be reduced to the case of 2 subalgorithms
described above, running it once for each of the ${k \choose 2}$ pairs. Each
of these runs passes parameters $\frac{\varepsilon}{4}$ and $\frac{\delta}{k+1}$
to the two subalgorithms, obtaining a KWIK bound of:
$$B(\varepsilon, \delta) = O\left( \frac{k}{\varepsilon^2} \ln \frac{k}{\delta}
\right) + \sum_{i=1}^k B_i \left( \frac{\varepsilon}{4}, \frac{\delta}{k + 1}
\right)$$


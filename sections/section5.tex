% TODO: Rename H to C

In this section, we will present a set of algorithms that, by combining KWIK
learners, are able to learn a more complex concept class. \\

\paragraph{Algorithm 6: Union Algorithm}
Let $H_i: X \to Y$ be a KWIK-learnable concept class by an algorithm $A_i$
with KWIK bound of $B_i(\varepsilon, \delta)$ for $1 \leq i \leq k$. Note that
all $H_1, \ldots, H_k$ share the same input and output spaces. \\

The union algorithm learns $H = \bigcup_i H_i$ with KWIK bound $B(\varepsilon,
\delta) = (k - 1) + \sum_{i = 1}^k B_i(\varepsilon, \delta)$. This can be
understood as a generalization of the Enumeration Algorithm (Algorithm 2). \\

The union algorithm maintains a set of ``potential algorithms'' $\hat{A}$,
initialized as $\{ A_1, \ldots, A_k \}$. For every received example
$x \in X$, it runs all algorithms in $\hat{A}$ to obtain a prediction from each
of them $\hat{y}_i$. It creates a set $\hat{L} = \{ \hat{y}_i | A_i \in \hat{A} \}$:
\begin{enumerate}
  \item If $\bot \in \hat{L}$, at least one of the algorithms doesn't know the
  answer with enough accuracy, then the union algorithm outputs $\bot$ and, once
  the correct label is received, it passes it to each of the algorithms that
  returned $\bot$.
  \item If $|\hat{L}| > 1$, at least two algorithms disagree on labeling $x$.
  Then, the union algorithm outputs $\bot$. Once the correct label $y$ is
  received, it eliminates from $\hat{A}$ the algorithms for which $\hat{y}_i(x)
  \neq y$.
  \item Otherwise, $|hat{L}| = 1$. This means that all the algorithms in
  $\hat{A}$ agree on labeling $x$. The union algorithm outputs the label in
  $\hat{L}$.
\end{enumerate}

The algorithm will only output $\bot$ in the first or second cases described
above:
\begin{itemize}
  \item In the first case, the algorithm will output at most $\sum_{i = 1}^k
  B_i(\varepsilon, \delta)$.
  \item In the second case, the union algorithm is going to eliminate at least one
  algorithm from $\hat{A}$. Initially, $|\hat{A}| = k$, so the algorithm will
  output $\bot$ at most $k - 1$ times and eliminates $k - 1$ algorithms from
  $\hat{A}$.
\end{itemize}

In total, the KWIK bound of the union algorithm is the sum of both cases:
$$ B(\varepsilon, \delta) = (k - 1) + \sum_{i = 1}^k B_i(\varepsilon, \delta) $$


\paragraph{Example 2}
Let:
\begin{itemize}
  \item $X = Y = \mathbb{R}$
  \item $H_1 = \{ f \ | \ f(x) =|x - c|, c \in \mathbb{R} \}$ KWIK learnable by $A_1$
  \item $H_2 = \{ f \ | \ f(x) = mx + b, m \in \mathbb{R}, b \in \mathbb{R} \}$ KWIK learnable by $A_2$
  \item $H = H_1 \cup H_2$
\end{itemize}

$H_1$ can be learned using a 1 dimension version of Planar-distance Algorithm (Algorithm 3) with a
KWIK bound of 2. $H_2$ can be learned with a KWIK bound of $2$,
the number of points needed to define a line.
$H$ is KWIK learnable using the union algorithm.
\begin{enumerate}
  \item Initially, $\hat{A} = \{A_1, A_2 \}$
  \item First, the algorithm receives $x_1 = 2$. $\hat{L} = \{ \bot \}$ because none of the algorithms knows the answer. Then, it outputs $\bot$ and receives $y_1 = 2$, which it passes to $A_1$, $A_2$.
  \item The algorithm receives $x_2 = 8$. $A_1$ and $A_2$ still don't know the actual value. Therefore, $\hat{L} = \{ \bot \}$ and it outputs $\bot$. Afterwards, it receives and passes $y_2 = 4$ to $A_1$ and $A_2$.
    Since $A_1$ and $A_2$ have returned $\bot$ twice, they have learned an hypothesis for every $x \in \mathbb{R}$.
    \begin{itemize}
      \item $A_1$ knows that $c = 4$. Because it's the only point that has a distance of $2$ from $x_1=2$ and 4 from $x_2 = 8$.
      \item $A_2$ learns the line that connects $(x_1, y_1)$ and $(x_2, y_2)$. Therefore, $m = 1/3$ and $b = 4/3$.
    \end{itemize}
  \item The algorithm receives $x_3 = 1$. $\hat{L} = \{ 3, 5/3 \}$
    \begin{itemize}
      \item $A_1$ predicts $3$
      \item $A_2$ predicts $5/3$
    \end{itemize}
    It returns $\bot$ because $A_1$ and $A_2$ disagree. Then it receives $y_3 = 3$ and eliminates algorithm $A_2$.
    From now on the algorithm will predict all the examples using $A_1$.
\end{enumerate}

\paragraph{Algorithm 7: Input-Partition Algorithm}
Let:
\begin{itemize}
  \item $X = \bigcup_{i=1}^k X_i$, where all $X_i \cap X_j = \emptyset \forall i \neq j$
  \item $H_i: X_i \to Y$ be a KWIK-learnable concept class by an algorithm $A_i$
  with KWIK bound of $B_i(\varepsilon, \delta)$ for $1 \leq i \leq k$. Note
  that all $H_1, \ldots, H_k$ share the same output space but their input spaces
  are disjoint.
  \item $H \subseteq X \to Y$ be a concept class.
\end{itemize}

The input-partition algorithm KWIK-learns $H$ with KWIK bound $B(\varepsilon,
\delta) = \sum_{i=1}^k B_i(\varepsilon, \delta/k)$. \\

When the algorithm receives an $x \in X_i$, it calls $A_i$ with parameters
$\varepsilon$ and $\delta/k$ and returns its response. By definition of $A_i$,
its response will be $\varepsilon$-accurate with probability at least $1 -
\delta/k$. Using union bounds, we know that the response over all $X$ of the
algorithm will be $\varepsilon$-accurate with probability at least $1 - \delta$.
Given that $X$ is the union of $k$ disjoint input spaces, the KWIK bound
of the input-partition algorithm will be $B(\varepsilon, \delta) = \sum_{i=1}^k
= (\varepsilon, \delta/k)$.

\paragraph{Example 3} Let $G$ be a Markov Decision Process consisting of $n$
states and $m$ actions. There are $n^2m$ transitions represented as triplets of
the form $(state_{origin}, action, state_{target})$. We want to learn the
probability of each transition to happen. By receiving Bernoulli observations,
we can KWIK-learn this using algorithm 7 (Input-Partition Algorithm) which
uses algorithm 4 (Coin-Learning Algorithm) as its $A_i$ for every $1 \leq i \leq
n^2m$ with parameters $\varepsilon$ and $\frac{\delta}{n^2m}$. The KWIK bound of
this algorithm is $B(\varepsilon, \delta) = n^2m \cdot O(\frac{1}{\varepsilon^2}
\ln \frac{n^2m}{\delta}) = O(\frac{n^2m}{\varepsilon^2} \ln \frac{nm}{\delta})$.
